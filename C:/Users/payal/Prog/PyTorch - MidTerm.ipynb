{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e34354",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial with Code Examples\n",
    "### Name: Payal Pitroda  \n",
    "### Library: PyTorch \n",
    "### URL: https://github.com/pitrodap/OIM_7502_classwork \n",
    "### Description:   \n",
    "This library is a popular open-source machine learning library for Python. It provides a flexible framework for building and training deep learning models. At its core, PyTorch provides two main features: An n-dimensional Tensor, similar to numpy but can run on GPUs and Automatic differentiation for building and training neural networks. It is widely used for various applications such as natural language processing (NLP), computer vision, and reinforcement learning. PyTorch also offers dynamic computational graphs along with automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd4f9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import math\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c2236",
   "metadata": {},
   "source": [
    "If you need direct instructions on installation, please use this link: \n",
    "https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdece01",
   "metadata": {},
   "source": [
    "## How to create a Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d0e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "z = torch.zeros(5, 3)\n",
    "print(z)\n",
    "print(z.dtype)\n",
    "\n",
    "i = torch.ones((5, 3), dtype=torch.int16)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7053f7",
   "metadata": {},
   "source": [
    "Here, we created a 5x3 matrix filled with zeros, and query its datatype to find out that the zeros are 32-bit floating point numbers, which is the default PyTorch. If you wanted integers instead, you can always override the default. You can also see that when we do change the default, the tensor helpfully reports this when printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3571e3",
   "metadata": {},
   "source": [
    "We can also initialize learning weights randomly, often with a specific seed for the PRNG for reproducibility of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83c085b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random tensor:\n",
      "tensor([[0.3126, 0.3791],\n",
      "        [0.3087, 0.0736]])\n",
      "\n",
      "A different random tensor:\n",
      "tensor([[0.4216, 0.0691],\n",
      "        [0.2332, 0.4047]])\n",
      "\n",
      "Should match r1:\n",
      "tensor([[0.3126, 0.3791],\n",
      "        [0.3087, 0.0736]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "r1 = torch.rand(2, 2)\n",
    "print('A random tensor:')\n",
    "print(r1)\n",
    "\n",
    "r2 = torch.rand(2, 2)\n",
    "print('\\nA different random tensor:')\n",
    "print(r2) # new values\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "r3 = torch.rand(2, 2)\n",
    "print('\\nShould match r1:')\n",
    "print(r3) # repeats values of r1 because of re-seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8485214",
   "metadata": {},
   "source": [
    "PyTorch tensors perform arithmetic operations intuitively. Tensors of similar shapes may be added, multiplied, etc. Operations with scalars are distributed over the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b39b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "torch.Size([2, 3])\n",
      "A random matrix, r:\n",
      "tensor([[ 0.9956, -0.2232],\n",
      "        [ 0.3858, -0.6593]])\n",
      "\n",
      "Absolute value of r:\n",
      "tensor([[0.9956, 0.2232],\n",
      "        [0.3858, 0.6593]])\n",
      "\n",
      "Inverse sine of r:\n",
      "tensor([[ 1.4775, -0.2251],\n",
      "        [ 0.3961, -0.7199]])\n",
      "\n",
      "Determinant of r:\n",
      "tensor(-0.5703)\n",
      "\n",
      "Singular value decomposition of r:\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.8353, -0.5497],\n",
      "        [-0.5497,  0.8353]]),\n",
      "S=tensor([1.1793, 0.4836]),\n",
      "V=tensor([[-0.8851, -0.4654],\n",
      "        [ 0.4654, -0.8851]]))\n",
      "\n",
      "Average and standard deviation of r:\n",
      "(tensor(0.7217), tensor(0.1247))\n",
      "\n",
      "Maximum value of r:\n",
      "tensor(0.9956)\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "twos = torch.ones(2, 3) * 2 # every element is multiplied by 2\n",
    "print(twos)\n",
    "\n",
    "threes = ones + twos       # addition allowed because shapes are similar\n",
    "print(threes)              # tensors are added element-wise\n",
    "print(threes.shape)        # this has the same dimensions as input tensors\n",
    "\n",
    "r1 = torch.rand(2, 3)\n",
    "r2 = torch.rand(3, 2)\n",
    "# uncomment this line to get a runtime error\n",
    "# r3 = r1 + r2\n",
    "torch.Size([2, 3])\n",
    "\n",
    "r = (torch.rand(2, 2) - 0.5) * 2 # values between -1 and 1\n",
    "print('A random matrix, r:')\n",
    "print(r)\n",
    "\n",
    "# Common mathematical operations are supported:\n",
    "print('\\nAbsolute value of r:')\n",
    "print(torch.abs(r))\n",
    "\n",
    "# ...as are trigonometric functions:\n",
    "print('\\nInverse sine of r:')\n",
    "print(torch.asin(r))\n",
    "\n",
    "# ...and linear algebra operations like determinant and singular value decomposition\n",
    "print('\\nDeterminant of r:')\n",
    "print(torch.det(r))\n",
    "print('\\nSingular value decomposition of r:')\n",
    "print(torch.svd(r))\n",
    "\n",
    "# ...and statistical and aggregate operations:\n",
    "print('\\nAverage and standard deviation of r:')\n",
    "print(torch.std_mean(r))\n",
    "print('\\nMaximum value of r:')\n",
    "print(torch.max(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ffe889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d48bb9",
   "metadata": {},
   "source": [
    "### Let's now learn more about Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a978cc4",
   "metadata": {},
   "source": [
    "Tensors can be created directly from data. The data type is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff018ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a0f97",
   "metadata": {},
   "source": [
    "Tensors can be created from NumPy arrays (and vice versa - see Bridge with NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6561042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e067165",
   "metadata": {},
   "source": [
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8291b05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.1384, 0.4759],\n",
      "        [0.7481, 0.0361]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719189a",
   "metadata": {},
   "source": [
    "With random or constant values shape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "588bd893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.5062, 0.8469, 0.2588],\n",
      "        [0.2707, 0.4115, 0.6839]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a147e",
   "metadata": {},
   "source": [
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ac4050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452da4e9",
   "metadata": {},
   "source": [
    "### Operations on Tensors \n",
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49924d2",
   "metadata": {},
   "source": [
    "Standard numpy-like indexing and slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e6f67ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5747bfa",
   "metadata": {},
   "source": [
    "Joining tensors You can use torch.cat to concatenate a sequence of tensors along a given dimension. See also torch.stack, another tensor joining operator that is subtly different from torch.cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9520347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58d360",
   "metadata": {},
   "source": [
    "Arithmetic operations -This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value ``tensor.T`` returns the transpose of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0d77b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c709f",
   "metadata": {},
   "source": [
    "This computes the element-wise product. z1, z2, z3 will have the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ba328c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914e09d",
   "metadata": {},
   "source": [
    "Single-element tensors If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using item():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9fb83ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a0169",
   "metadata": {},
   "source": [
    "In-place operations Operations that store the result into the operand are called in-place. They are denoted by a _ suffix. For example: x.copy_(y), x.t_(), will change x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6171542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d9671",
   "metadata": {},
   "source": [
    "### Working with Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4b084",
   "metadata": {},
   "source": [
    "The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO. We use the FashionMNIST dataset. Every TorchVision Dataset includes two arguments: transform and target_transform to modify the samples and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e121c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2de5074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48550efa",
   "metadata": {},
   "source": [
    "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f16aebf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb33fd",
   "metadata": {},
   "source": [
    "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU or MPS if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17e02574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000d4bf",
   "metadata": {},
   "source": [
    "Optimizing the Model Parameters\n",
    "To train a model, we need a loss function and an optimizer. In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters. We also check the model’s performance against the test dataset to ensure it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eeac8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9920a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02eeafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596072bc",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dd9a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.297716  [   64/60000]\n",
      "loss: 2.296212  [ 6464/60000]\n",
      "loss: 2.278649  [12864/60000]\n",
      "loss: 2.276129  [19264/60000]\n",
      "loss: 2.249752  [25664/60000]\n",
      "loss: 2.224671  [32064/60000]\n",
      "loss: 2.232621  [38464/60000]\n",
      "loss: 2.200275  [44864/60000]\n",
      "loss: 2.207772  [51264/60000]\n",
      "loss: 2.166907  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.9%, Avg loss: 2.165901 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.176962  [   64/60000]\n",
      "loss: 2.175058  [ 6464/60000]\n",
      "loss: 2.121765  [12864/60000]\n",
      "loss: 2.136795  [19264/60000]\n",
      "loss: 2.083737  [25664/60000]\n",
      "loss: 2.026645  [32064/60000]\n",
      "loss: 2.058328  [38464/60000]\n",
      "loss: 1.987872  [44864/60000]\n",
      "loss: 2.000674  [51264/60000]\n",
      "loss: 1.916838  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 1.916489 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.952897  [   64/60000]\n",
      "loss: 1.926626  [ 6464/60000]\n",
      "loss: 1.810859  [12864/60000]\n",
      "loss: 1.846733  [19264/60000]\n",
      "loss: 1.740672  [25664/60000]\n",
      "loss: 1.679634  [32064/60000]\n",
      "loss: 1.706824  [38464/60000]\n",
      "loss: 1.610285  [44864/60000]\n",
      "loss: 1.634104  [51264/60000]\n",
      "loss: 1.513015  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 1.530598 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.603319  [   64/60000]\n",
      "loss: 1.563424  [ 6464/60000]\n",
      "loss: 1.408997  [12864/60000]\n",
      "loss: 1.476226  [19264/60000]\n",
      "loss: 1.356857  [25664/60000]\n",
      "loss: 1.343310  [32064/60000]\n",
      "loss: 1.361049  [38464/60000]\n",
      "loss: 1.291262  [44864/60000]\n",
      "loss: 1.325900  [51264/60000]\n",
      "loss: 1.212456  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.241966 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.324530  [   64/60000]\n",
      "loss: 1.303551  [ 6464/60000]\n",
      "loss: 1.135746  [12864/60000]\n",
      "loss: 1.239870  [19264/60000]\n",
      "loss: 1.113376  [25664/60000]\n",
      "loss: 1.133952  [32064/60000]\n",
      "loss: 1.158777  [38464/60000]\n",
      "loss: 1.103323  [44864/60000]\n",
      "loss: 1.141940  [51264/60000]\n",
      "loss: 1.049782  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.074209 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429fe38",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eab1f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8aa97",
   "metadata": {},
   "source": [
    "Loading Models\n",
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0462ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c86f99",
   "metadata": {},
   "source": [
    "This model can now be used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82557dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b2a69",
   "metadata": {},
   "source": [
    "### NumPy and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8dbc60a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.989\n",
      "1.005\n"
     ]
    }
   ],
   "source": [
    "#What we could do in NumPy\n",
    "l = np.random.normal(loc=10, size=50)\n",
    "print(f\"{l.mean():.3f}\")\n",
    "print(f\"{l.std():.3f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45281cd4",
   "metadata": {},
   "source": [
    "This code generates an array l of 50 random numbers drawn from a normal distribution with mean 10 (loc=10) and a default standard deviation of 1. Then it prints out the mean and standard deviation of this array rounded to three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50f710f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.039\n",
      "1.010\n"
     ]
    }
   ],
   "source": [
    "#Lets do the same thing using Pytorch\n",
    "# Generate random numbers from a normal distribution\n",
    "l = torch.normal(mean=10, std=1, size=(50,))\n",
    "\n",
    "# Calculate and print mean\n",
    "print(f\"{l.mean().item():.3f}\")\n",
    "\n",
    "# Calculate and print standard deviation\n",
    "print(f\"{l.std().item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94763ee",
   "metadata": {},
   "source": [
    "This code accomplishes the same task as the NumPy code but using PyTorch tensors. It generates 50 random numbers drawn from a normal distribution with mean 10 and standard deviation 1, calculates their mean and standard deviation using PyTorch tensor methods, and prints the results rounded to three decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741307ae",
   "metadata": {},
   "source": [
    "### Trying with a model - NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e10796",
   "metadata": {},
   "source": [
    "This code performs a simple polynomial regression using gradient descent to minimize the loss function. Here's a breakdown of what it does:\n",
    "\n",
    "-It generates random input x values ranging from -π to π with 2000 data points and calculates the corresponding sine values y.\n",
    "\n",
    "-Randomly initializes the coefficients a, b, c, and d.\n",
    "\n",
    "-Sets a learning rate.\n",
    "\n",
    "-Enters a loop of 2000 iterations for gradient descent optimization.\n",
    "\n",
    "-In each iteration:\n",
    "    \n",
    "    Computes the predicted y values (y_pred) using the current coefficients.\n",
    "    \n",
    "    Calculates the loss function, which is the sum of squared differences between predicted y and actual y.\n",
    "    \n",
    "    Prints the loss every 100 iterations.\n",
    "    \n",
    "    Computes gradients of the loss function with respect to the coefficients (a, b, c, d).\n",
    "    \n",
    "    Updates the coefficients using gradient descent: subtracts the product of the learning rate and the respective gradient from each coefficient.\n",
    "\n",
    "-After the loop, it prints the final result with the learned coefficients for the polynomial equation.\n",
    "\n",
    "So essentially, it's fitting a polynomial curve of degree 3 (a + b*x + c*x^2 + d*x^3) to approximate the sine function over the given range using gradient descent optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c37084bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 347.5586995439629\n",
      "199 246.53144041330597\n",
      "299 175.71781745736752\n",
      "399 126.05302614856004\n",
      "499 91.20295086009364\n",
      "599 66.73649608174503\n",
      "699 49.551879328112406\n",
      "799 37.476528060498666\n",
      "899 28.987835290541824\n",
      "999 23.018123516719303\n",
      "1099 18.81832763399651\n",
      "1199 15.862654354654904\n",
      "1299 13.781857806935205\n",
      "1399 12.316513541697756\n",
      "1499 11.284277894790847\n",
      "1599 10.556933980089006\n",
      "1699 10.044290386451157\n",
      "1799 9.682881049903365\n",
      "1899 9.428030845692025\n",
      "1999 9.248281884135942\n",
      "Result: y = 0.021550341351434122 + 0.860702176296195 x + -0.003717792482350343 x^2 + -0.09389385419589356 x^3\n"
     ]
    }
   ],
   "source": [
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d0741",
   "metadata": {},
   "source": [
    "### Trying with a model - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59ccaf",
   "metadata": {},
   "source": [
    "This code performs a similar task as the previous one, but it's implemented using PyTorch tensors for automatic differentiation, which enables easier backpropagation. Here's what the code does:\n",
    "\n",
    "-It sets the data type (dtype) to torch.float and the device to run on (device) to CPU.\n",
    "\n",
    "-It generates random input x values ranging from -π to π with 2000 data points and calculates the corresponding sine values y. These tensors are created on the specified device.\n",
    "\n",
    "-Randomly initializes the coefficients a, b, c, and d as tensors on the specified device.\n",
    "\n",
    "-Sets a learning rate.\n",
    "\n",
    "-Enters a loop of 2000 iterations for gradient descent optimization.\n",
    "\n",
    "-In each iteration:\n",
    "    \n",
    "    Computes the predicted y values (y_pred) using the current coefficients.\n",
    "    \n",
    "    Calculates the mean squared error loss.\n",
    "    \n",
    "    Prints the loss every 100 iterations.\n",
    "    \n",
    "    Computes gradients of the loss function with respect to the coefficients (a, b, c, d) using automatic differentiation.\n",
    "    \n",
    "    Updates the coefficients using gradient descent.\n",
    "    \n",
    "-After the loop, it prints the final result with the learned coefficients for the polynomial equation.\n",
    "\n",
    "This code essentially performs polynomial regression using PyTorch tensors and automatic differentiation, achieving the same goal as the NumPy code but with the added benefits of GPU acceleration and automatic gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4b8243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1770.32763671875\n",
      "199 1191.91943359375\n",
      "299 804.0960693359375\n",
      "399 543.8612060546875\n",
      "499 369.101318359375\n",
      "599 251.64505004882812\n",
      "699 172.63504028320312\n",
      "799 119.44034576416016\n",
      "899 83.59333801269531\n",
      "999 59.413963317871094\n",
      "1099 43.08884811401367\n",
      "1199 32.05574035644531\n",
      "1299 24.59162139892578\n",
      "1399 19.53670310974121\n",
      "1499 16.109737396240234\n",
      "1599 13.783905029296875\n",
      "1699 12.203714370727539\n",
      "1799 11.12890625\n",
      "1899 10.397041320800781\n",
      "1999 9.898110389709473\n",
      "Result: y = -0.024886634200811386 + 0.8344007134437561 x + 0.004293360281735659 x^2 + -0.09015269577503204 x^3\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62089b86",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
